Namespace(arch='resnet18', cuda='0', root='D:/qam64_awgn_sym128_sps16_esno20.dat')
ResNet(
  (conv1): Conv1d(2, 64, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace)
  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (fc): Linear(in_features=512, out_features=2, bias=True)
)
loading data
  epoch    accuracy    train_loss    valid_loss    cp       dur
-------  ----------  ------------  ------------  ----  --------
      1      0.9436        0.5189        0.1561     +  119.5611
      2      0.9813        0.0832        0.0521     +  117.8088
      3      0.9890        0.0370        0.0317     +  117.9248
      4      0.9914        0.0233        0.0245     +  118.3842
      5      0.9941        0.0166        0.0189     +  118.1100
      6      0.9944        0.0124        0.0166     +  118.0379
      7      0.9949        0.0098        0.0148     +  118.0109
      8      0.9954        0.0079        0.0137     +  118.0830
      9      0.9954        0.0064        0.0131     +  118.1050
     10      0.9959        0.0053        0.0130     +  118.2721
     11      0.9956        0.0046        0.0125     +  118.6904
     12      0.9958        0.0038        0.0117     +  118.4563
     13      0.9959        0.0034        0.0119        118.2801
     14      0.9961        0.0029        0.0112     +  118.3812
     15      0.9960        0.0025        0.0112        118.2171
     16      0.9959        0.0022        0.0108     +  118.2721
     17      0.9962        0.0019        0.0106     +  118.3511
     18      0.9960        0.0018        0.0111        118.0009
     19      0.9960        0.0016        0.0105     +  117.7367
     20      0.9962        0.0014        0.0108        117.9989
     21      0.9957        0.0014        0.0124        117.5966
     22      0.9962        0.0013        0.0101     +  118.7364
     23      0.9963        0.0012        0.0102        118.0499
     24      0.9964        0.0011        0.0103        117.9018
     25      0.9962        0.0010        0.0103        118.0229
     26      0.9963        0.0009        0.0103        117.9809
Re-initializing optimizer because the following parameters were re-set: lr.
     27      0.9962        0.0008        0.0107        118.3482
     28      0.9964        0.0008        0.0101        118.8706
     29      0.9964        0.0008        0.0102        118.7234
     30      0.9964        0.0007        0.0102        117.9609
     31      0.9964        0.0008        0.0101     +  118.0309
     32      0.9964        0.0008        0.0101        117.9419
     33      0.9963        0.0007        0.0102        117.7888
     34      0.9963        0.0007        0.0103        117.5846
     35      0.9964        0.0008        0.0104        117.8158
Re-initializing optimizer because the following parameters were re-set: lr.
     36      0.9963        0.0008        0.0104        117.6836
     37      0.9964        0.0007        0.0101        118.0289
     38      0.9963        0.0008        0.0103        117.9339
     39      0.9964        0.0007        0.0100     +  117.6576
     40      0.9964        0.0008        0.0101        117.8208
     41      0.9964        0.0007        0.0102        117.8778
     42      0.9965        0.0008        0.0101        118.3801
     43      0.9963        0.0007        0.0103        118.6434
Re-initializing optimizer because the following parameters were re-set: lr.
     44      0.9963        0.0008        0.0101        118.7224
     45      0.9964        0.0007        0.0101        119.7542
     46      0.9964        0.0008        0.0101        118.3102
     47      0.9964        0.0007        0.0100        118.7645
     48      0.9964        0.0008        0.0101        120.6218
     49      0.9964        0.0007        0.0101        120.9661
     50      0.9964        0.0007        0.0100        120.9391
     51      0.9964        0.0008        0.0102        120.1215
     52      0.9964        0.0008        0.0101        120.3847
     53      0.9964        0.0007        0.0101        119.8032
     54      0.9963        0.0007        0.0102        119.6751
     55      0.9963        0.0007        0.0102        119.1277
     56      0.9964        0.0008        0.0101        118.8376
     57      0.9964        0.0008        0.0101        118.5323
     58      0.9963        0.0007        0.0102        118.8615
Stopping since valid_loss has not improved in the last 20 epochs.
Best Model State Restored
Best Confusion Matrix:
 [[100   0]
 [  0 100]]
