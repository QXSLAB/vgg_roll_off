Namespace(arch='resnet18', cuda='3', root='D:/dqpsk_awgn_sym128_sps16_esno18.dat')
ResNet(
  (conv1): Conv1d(2, 64, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace)
  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (fc): Linear(in_features=512, out_features=2, bias=True)
)
loading data
  epoch    accuracy    train_loss    valid_loss    cp       dur
-------  ----------  ------------  ------------  ----  --------
      1      0.9460        0.5259        0.1507     +  122.3872
      2      0.9829        0.0820        0.0486     +  120.1395
      3      0.9886        0.0366        0.0310     +  120.3157
      4      0.9905        0.0238        0.0259     +  120.4367
      5      0.9924        0.0169        0.0204     +  120.6499
      6      0.9922        0.0130        0.0200     +  120.5138
      7      0.9935        0.0101        0.0174     +  120.5498
      8      0.9942        0.0082        0.0150     +  120.8010
      9      0.9943        0.0067        0.0140     +  120.6529
     10      0.9948        0.0057        0.0132     +  120.4217
     11      0.9950        0.0044        0.0127     +  120.6789
     12      0.9952        0.0039        0.0126     +  120.8531
     13      0.9950        0.0033        0.0126        120.7009
     14      0.9953        0.0029        0.0121     +  120.6728
     15      0.9952        0.0025        0.0118     +  120.7069
     16      0.9954        0.0022        0.0116     +  120.7069
     17      0.9957        0.0019        0.0111     +  121.2913
     18      0.9958        0.0018        0.0111        121.1543
     19      0.9957        0.0016        0.0110     +  120.9111
     20      0.9958        0.0013        0.0109     +  121.1663
     21      0.9958        0.0014        0.0108     +  121.0322
     22      0.9961        0.0011        0.0106     +  121.0242
     23      0.9960        0.0011        0.0104     +  121.2564
     24      0.9958        0.0010        0.0106        120.9781
     25      0.9961        0.0009        0.0105        121.4835
     26      0.9960        0.0009        0.0103     +  121.0961
     27      0.9961        0.0008        0.0102     +  120.9861
     28      0.9960        0.0008        0.0103        121.2313
     29      0.9961        0.0007        0.0102        121.5065
     30      0.9962        0.0007        0.0101     +  121.2914
     31      0.9962        0.0006        0.0100     +  121.2793
     32      0.9963        0.0006        0.0100     +  121.3944
     33      0.9963        0.0006        0.0100        121.5515
     34      0.9963        0.0005        0.0101        121.5245
     35      0.9964        0.0006        0.0102        121.5685
     36      0.9964        0.0005        0.0099     +  121.5745
     37      0.9964        0.0005        0.0098     +  121.4354
     38      0.9963        0.0005        0.0099        121.4915
     39      0.9964        0.0005        0.0098     +  121.5555
     40      0.9963        0.0004        0.0106        121.5386
     41      0.9963        0.0004        0.0098        121.5966
     42      0.9963        0.0005        0.0111        121.2003
     43      0.9966        0.0004        0.0097     +  121.3925
     44      0.9964        0.0004        0.0098        121.4685
     45      0.9964        0.0004        0.0099        121.3854
     46      0.9965        0.0003        0.0103        121.6547
     47      0.9966        0.0004        0.0099        121.4324
Re-initializing optimizer because the following parameters were re-set: lr.
     48      0.9966        0.0004        0.0097        121.3424
     49      0.9966        0.0003        0.0101        121.6326
     50      0.9966        0.0003        0.0096     +  121.5826
     51      0.9967        0.0003        0.0097        121.4885
     52      0.9966        0.0003        0.0097        121.4914
     53      0.9966        0.0003        0.0096        121.4274
     54      0.9966        0.0003        0.0096        121.5465
Re-initializing optimizer because the following parameters were re-set: lr.
     55      0.9966        0.0003        0.0097        121.5636
     56      0.9966        0.0003        0.0096     +  121.5325
     57      0.9966        0.0003        0.0096        121.4284
     58      0.9966        0.0003        0.0098        121.6366
     59      0.9966        0.0003        0.0096        121.7036
     60      0.9966        0.0003        0.0095     +  121.5075
     61      0.9965        0.0003        0.0096        121.6796
     62      0.9966        0.0003        0.0096        121.6776
     63      0.9966        0.0003        0.0096        121.3815
     64      0.9966        0.0003        0.0096        121.6727
Re-initializing optimizer because the following parameters were re-set: lr.
     65      0.9966        0.0003        0.0096        121.5606
     66      0.9966        0.0003        0.0096        121.3463
     67      0.9966        0.0003        0.0096        121.7447
     68      0.9966        0.0003        0.0096        121.6696
     69      0.9966        0.0003        0.0096        121.9078
     70      0.9966        0.0003        0.0096        121.1142
     71      0.9966        0.0003        0.0096        121.3964
     72      0.9966        0.0003        0.0096        121.4654
     73      0.9966        0.0003        0.0096        120.7409
     74      0.9967        0.0003        0.0097        121.2804
     75      0.9968        0.0003        0.0097        120.1725
     76      0.9967        0.0003        0.0097        120.0374
     77      0.9966        0.0003        0.0096        120.1575
     78      0.9966        0.0003        0.0096        120.6909
     79      0.9966        0.0003        0.0096        120.4867
Stopping since valid_loss has not improved in the last 20 epochs.
Best Model State Restored
Best Confusion Matrix:
 [[100   0]
 [  0 100]]
