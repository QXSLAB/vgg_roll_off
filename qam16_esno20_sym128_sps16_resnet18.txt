Namespace(arch='resnet18', cuda='1', root='D:/qam16_awgn_sym128_sps16_esno20.dat')
ResNet(
  (conv1): Conv1d(2, 64, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace)
  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (fc): Linear(in_features=512, out_features=2, bias=True)
)
loading data
  epoch    accuracy    train_loss    valid_loss    cp       dur
-------  ----------  ------------  ------------  ----  --------
      1      0.9561        0.4717        0.1230     +  127.8512
      2      0.9836        0.0755        0.0484     +  126.1730
      3      0.9897        0.0364        0.0298     +  126.1069
      4      0.9921        0.0228        0.0231     +  126.1920
      5      0.9938        0.0161        0.0187     +  126.6203
      6      0.9942        0.0120        0.0168     +  127.7001
      7      0.9950        0.0097        0.0148     +  127.3249
      8      0.9957        0.0076        0.0135     +  127.2658
      9      0.9957        0.0063        0.0130     +  127.3358
     10      0.9959        0.0052        0.0123     +  127.4910
     11      0.9958        0.0043        0.0121     +  127.2288
     12      0.9956        0.0037        0.0122        127.2678
     13      0.9962        0.0031        0.0111     +  127.1507
     14      0.9962        0.0026        0.0110     +  127.3589
     15      0.9962        0.0026        0.0108     +  122.7495
     16      0.9962        0.0020        0.0105     +  121.9528
     17      0.9966        0.0019        0.0102     +  122.0129
     18      0.9962        0.0015        0.0110        121.0832
     19      0.9966        0.0015        0.0100     +  120.9661
     20      0.9963        0.0014        0.0101        122.5943
     21      0.9963        0.0013        0.0101        120.7049
     22      0.9964        0.0012        0.0105        121.2883
     23      0.9966        0.0011        0.0099     +  120.2676
     24      0.9966        0.0011        0.0095     +  120.7039
     25      0.9966        0.0010        0.0095        121.2944
     26      0.9965        0.0009        0.0102        121.0772
     27      0.9966        0.0009        0.0096        121.5095
     28      0.9967        0.0008        0.0095        120.3676
Re-initializing optimizer because the following parameters were re-set: lr.
     29      0.9964        0.0008        0.0097        120.2105
     30      0.9968        0.0007        0.0094     +  120.3817
     31      0.9967        0.0006        0.0093     +  120.4267
     32      0.9968        0.0007        0.0093        120.3857
     33      0.9967        0.0007        0.0093     +  120.4658
     34      0.9967        0.0007        0.0093        120.5408
     35      0.9967        0.0006        0.0093        120.5537
     36      0.9968        0.0006        0.0094        120.3777
     37      0.9967        0.0006        0.0093     +  120.1955
     38      0.9968        0.0007        0.0093        120.3817
     39      0.9968        0.0006        0.0093     +  120.4047
     40      0.9967        0.0007        0.0098        120.4568
     41      0.9967        0.0007        0.0094        120.5648
     42      0.9967        0.0006        0.0098        120.4898
     43      0.9967        0.0006        0.0096        120.3986
Re-initializing optimizer because the following parameters were re-set: lr.
     44      0.9967        0.0007        0.0094        120.3407
     45      0.9968        0.0006        0.0093        120.4717
     46      0.9967        0.0006        0.0093        120.4818
     47      0.9968        0.0006        0.0093        120.4677
     48      0.9968        0.0006        0.0095        120.4857
     49      0.9967        0.0006        0.0094        120.4137
     50      0.9967        0.0006        0.0096        120.4887
     51      0.9967        0.0006        0.0094        120.3936
     52      0.9968        0.0006        0.0093        120.5638
     53      0.9967        0.0006        0.0094        120.5638
     54      0.9968        0.0006        0.0093        120.5818
     55      0.9968        0.0006        0.0093        120.4998
     56      0.9968        0.0006        0.0093        120.5759
     57      0.9968        0.0006        0.0095        120.6228
     58      0.9968        0.0006        0.0095        120.6108
Stopping since valid_loss has not improved in the last 20 epochs.
Best Model State Restored
Best Confusion Matrix:
 [[100   0]
 [  0 100]]
