Namespace(arch='resnet18', cuda='2', root='D:/oqpsk_awgn_sym128_sps16_esno18.dat')
ResNet(
  (conv1): Conv1d(2, 64, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace)
  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (fc): Linear(in_features=512, out_features=2, bias=True)
)
loading data
  epoch    accuracy    train_loss    valid_loss    cp       dur
-------  ----------  ------------  ------------  ----  --------
      1      0.9932        0.1552        0.0257     +  120.2285
      2      0.9968        0.0169        0.0115     +  121.2223
      3      0.9974        0.0110        0.0092     +  120.3626
      4      0.9976        0.0086        0.0080     +  120.6308
      5      0.9977        0.0074        0.0075     +  121.1912
      6      0.9979        0.0065        0.0064     +  120.6608
      7      0.9980        0.0059        0.0063     +  121.0931
      8      0.9980        0.0053        0.0059     +  119.5160
      9      0.9982        0.0049        0.0056     +  119.6472
     10      0.9982        0.0045        0.0055     +  119.6011
     11      0.9981        0.0041        0.0055     +  119.6501
     12      0.9983        0.0039        0.0052     +  119.7492
     13      0.9982        0.0036        0.0052        119.5891
     14      0.9983        0.0034        0.0051     +  119.6701
     15      0.9983        0.0030        0.0050     +  119.5110
     16      0.9984        0.0030        0.0051        119.6271
     17      0.9983        0.0027        0.0048     +  119.6632
     18      0.9983        0.0027        0.0049        119.6041
     19      0.9983        0.0025        0.0049        119.5870
     20      0.9985        0.0023        0.0046     +  119.6070
     21      0.9984        0.0023        0.0046        119.7822
     22      0.9984        0.0021        0.0045     +  119.8613
     23      0.9983        0.0019        0.0045        119.7402
     24      0.9984        0.0018        0.0046        119.5920
     25      0.9983        0.0017        0.0046        119.7292
     26      0.9985        0.0017        0.0045     +  119.5110
     27      0.9986        0.0015        0.0044     +  119.4789
     28      0.9985        0.0016        0.0045        119.6061
     29      0.9985        0.0014        0.0044        119.5430
     30      0.9985        0.0013        0.0044        119.6661
     31      0.9985        0.0013        0.0043     +  119.3709
     32      0.9983        0.0012        0.0047        119.4320
     33      0.9985        0.0012        0.0044        119.3119
     34      0.9983        0.0011        0.0046        119.4430
     35      0.9986        0.0010        0.0043     +  119.3969
     36      0.9982        0.0011        0.0046        119.5010
     37      0.9985        0.0010        0.0042     +  119.4430
     38      0.9984        0.0009        0.0044        119.6261
     39      0.9985        0.0010        0.0043        119.4769
     40      0.9983        0.0009        0.0047        119.6791
     41      0.9985        0.0008        0.0042     +  119.4169
     42      0.9985        0.0008        0.0043        119.4520
     43      0.9985        0.0008        0.0042        119.4810
     44      0.9984        0.0008        0.0044        119.5391
     45      0.9985        0.0008        0.0042        119.4549
     46      0.9985        0.0008        0.0042     +  119.4639
     47      0.9986        0.0008        0.0042        119.4420
     48      0.9985        0.0007        0.0041     +  119.4850
     49      0.9984        0.0007        0.0045        119.6881
     50      0.9985        0.0006        0.0041        119.5600
     51      0.9985        0.0006        0.0042        119.6091
     52      0.9985        0.0006        0.0043        119.5670
Re-initializing optimizer because the following parameters were re-set: lr.
     53      0.9985        0.0006        0.0041        119.5961
     54      0.9986        0.0006        0.0041        119.7411
     55      0.9985        0.0006        0.0041        119.6601
     56      0.9986        0.0006        0.0041     +  119.5010
     57      0.9986        0.0006        0.0041        119.6130
     58      0.9985        0.0006        0.0042        119.5190
     59      0.9985        0.0005        0.0042        119.5631
     60      0.9985        0.0006        0.0042        119.4980
Re-initializing optimizer because the following parameters were re-set: lr.
     61      0.9985        0.0005        0.0041        119.5311
     62      0.9984        0.0005        0.0046        119.5320
     63      0.9985        0.0005        0.0042        119.6611
     64      0.9986        0.0006        0.0041        119.5540
     65      0.9984        0.0006        0.0043        119.4689
     66      0.9985        0.0005        0.0041        119.5290
     67      0.9984        0.0005        0.0043        119.4640
     68      0.9986        0.0006        0.0041        119.4600
     69      0.9985        0.0005        0.0041        119.4980
     70      0.9985        0.0005        0.0041        119.6981
     71      0.9984        0.0005        0.0041        119.5891
     72      0.9986        0.0005        0.0041        119.5260
     73      0.9984        0.0006        0.0041        119.5050
     74      0.9985        0.0006        0.0042        119.3079
     75      0.9985        0.0005        0.0042        119.4810
Stopping since valid_loss has not improved in the last 20 epochs.
Best Model State Restored
Best Confusion Matrix:
 [[100   0]
 [  0 100]]
